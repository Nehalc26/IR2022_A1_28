{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 \n",
    "### Data fetching & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('Humor,Hist,Media,Food')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import joblib\n",
    "from natsort import natsorted\n",
    "import string\n",
    "import regex as re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# give the path to dir of dataset.\n",
    "path =Path(\"./Humor,Hist,Media,Food\")\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import joblib\n",
    "#initialize the variables neede\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "fileno = 0\n",
    "positionalIndex ={}\n",
    "fileNumberMap={}\n",
    "    \n",
    "#read the files from the folder in alphabetical order.\n",
    "file_names = natsorted(os.listdir(path))\n",
    "for file in file_names:\n",
    "  #open the file in utf8 format\n",
    "    f = open(str(path)+\"/\"+file,'r', encoding =\"utf8\", errors =\"surrogateescape\")\n",
    "    #spli the lines for any multiple new lines\n",
    "    data = f.read().split('\\n\\n')\n",
    "    #initialize the position of each word in the file\n",
    "    pos=1\n",
    "    \"\"\"for every line in the file \n",
    "       make the string to lowercase,\n",
    "       remove any numbers from the line \n",
    "       make a tokenizer to convert the string to tokens, here used the regular expression tokenizer that removes the special characters\n",
    "       removed the stop words from the tokens.\"\"\"\n",
    "    for line in data: \n",
    "      line = line.lower()\n",
    "      line= re.sub(r'\\d+','',line)\n",
    "      tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "      tokenList = tokenizer.tokenize(line)\n",
    "      tokensWithoutStopWords = [word for word in tokenList if not word in stopwords.words('english')]\n",
    "      \"\"\" for each word in tokens, stem the words(remove the trailing characters) \n",
    "      \"\"\"\n",
    "      for word in tokensWithoutStopWords:\n",
    "        word= stemmer.stem(word)\n",
    "        if word in positionalIndex:\n",
    "          if fileno in positionalIndex[word]:\n",
    "            positionalIndex[word][fileno].append(pos)\n",
    "          else:\n",
    "            positionalIndex[word][fileno] = [pos]\n",
    "        else:\n",
    "          positionalIndex[word]={fileno:[pos]}\n",
    "          # positionalIndex[word]\n",
    "        pos+=1\n",
    "    fileNumberMap[fileno] = file\n",
    "\n",
    "\n",
    "    fileno+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fileNumberMap\n",
    "joblib.dump(positionalIndex,\"positionalIndex.joblib\")\n",
    "joblib.dump(fileNumberMap,\"FileNumberMap.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def phraseQuery(line):\n",
    "#     posIndex = joblib.load(\"positionalIndex.joblib\")\n",
    "#     fileNumberMap = joblib.load(\"FileNumberMap.joblib\")\n",
    "\n",
    "#     # for word in phrase: preprocess the query.\n",
    "#     line = line.lower()\n",
    "#     line= re.sub(r'\\d+','',line)\n",
    "#     line =re.sub(r'[^\\w\\s]','',line)\n",
    "#     tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "#     tokenList = tokenizer.tokenize(line)\n",
    "#     tokensWithoutStopWords = [word for word in tokenList if not word in stopwords.words('english')]\n",
    "\n",
    "#     print(\"Words To Find------->\\t\",tokensWithoutStopWords)\n",
    "    \n",
    "#     fileNames = []\n",
    "\n",
    "#     posList =[]\n",
    "#     indexForWord = {}\n",
    "#     \"\"\"for every word in the tokens, iterate over the positional Index and get the file number and positions for a word.\n",
    "#         Make a smaller dictionary containing only the files and position for the given words.\n",
    "#     \"\"\"\n",
    "#     for pos,word in enumerate(tokensWithoutStopWords):\n",
    "#         for docId,positions in posIndex[PorterStemmer().stem(word)].items():\n",
    "\n",
    "#             if docId not in indexForWord:\n",
    "#                 indexForWord[docId]=positions\n",
    "#             else:\n",
    "#                 for i in positions:\n",
    "#                     indexForWord[docId].append(i)\n",
    "    \n",
    "#     todel =[]\n",
    "#     \"\"\"For the file numbers and positions in the dictionary, remove the file number that contains positions less than the number of words in the query. \n",
    "#         make a list that contains files with the same and pop them from the dictionary.\"\"\"\n",
    "#     for k,v in indexForWord.items():\n",
    "#         if(len(indexForWord[k])<len(tokensWithoutStopWords)):\n",
    "#             todel.append(k)\n",
    "\n",
    "#     for i in todel:\n",
    "#         indexForWord.pop(i)\n",
    "\n",
    "#     \"\"\"Iterate over the file numbers and the positions in dictionary, sort the positions and check for consecutive positions.\n",
    "#         If the number of consecutive postions is not empty then the file contains the query.\"\"\"\n",
    "#     for docId,positions in indexForWord.items():\n",
    "#         positions.sort()\n",
    "#         # print(positions)\n",
    "#         consecutiveList = []\n",
    "#         for i in range(len(positions)-1):\n",
    "#             if(positions[i+1]-positions[i] ==1):\n",
    "#                 consecutiveList.append(positions[i+1]-positions[i])\n",
    "\n",
    "#         if(len(consecutiveList)):\n",
    "#             fileNames.append([docId,fileNumberMap[docId]])\n",
    "    \n",
    "#         # print(consecutiveList)\n",
    "#     print(\"Number Of Docs Retreived------>\\t\",len(fileNames),\"\\nFound  DocID & Name ------>\\t\",fileNames)\n",
    "\n",
    "\n",
    "# phraseQuery(\"These artists are \\\"cultural jammers\\\"\")\n",
    "\n",
    "\n",
    "# phraseQuery(\"laughing at your husband because \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words To Find------->\t ['artists', 'cultural', 'jammers']\n",
      "Number Of Docs Retreived------>\t 1 \n",
      "Found  DocID & Name ------>\t [[650, 'merry.txt']]\n",
      "Words To Find------->\t ['laughing', 'husband']\n",
      "Number Of Docs Retreived------>\t 2 \n",
      "Found  DocID & Name ------>\t [[418, 'gd_gal.txt'], [659, 'misery.hum']]\n",
      "Words To Find------->\t ['maiden']\n",
      "Number Of Docs Retreived------>\t 12 \n",
      "Found  DocID & Name ------>\t [[129, 'bnbeg2.4.txt'], [245, 'comic_st.gui'], [295, 'devils.jok'], [318, 'dromes.txt'], [345, 'epi_bnb.txt'], [547, 'jokes'], [549, 'jokes.txt'], [593, 'limerick.jok'], [889, 'reeves.txt'], [978, 'strine.txt'], [1035, 'top10st1.txt'], [1119, 'x-drinks.txt']]\n"
     ]
    }
   ],
   "source": [
    "def phraseQuery(line):\n",
    "    posIndex = joblib.load(\"positionalIndex.joblib\")\n",
    "    fileNumberMap = joblib.load(\"FileNumberMap.joblib\")\n",
    "\n",
    "    # for word in phrase: preprocess the query.\n",
    "    line = line.lower()\n",
    "    line= re.sub(r'\\d+','',line)\n",
    "    line =re.sub(r'[^\\w\\s]','',line)\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    tokenList = tokenizer.tokenize(line)\n",
    "    tokensWithoutStopWords = [word for word in tokenList if not word in stopwords.words('english')]\n",
    "\n",
    "    print(\"Words To Find------->\\t\",tokensWithoutStopWords)\n",
    "    \n",
    "    fileNames = []\n",
    "\n",
    "    posList =[]\n",
    "    indexForWord = {}\n",
    "    \"\"\"If we only have one word to find\"\"\"\n",
    "    if(len(tokensWithoutStopWords)==1): \n",
    "        for docId,positions in posIndex[PorterStemmer().stem(tokensWithoutStopWords[0])].items():\n",
    "            fileNames.append([docId,fileNumberMap[docId]])\n",
    "        print(\"Number Of Docs Retreived------>\\t\",len(fileNames),\"\\nFound  DocID & Name ------>\\t\",fileNames)\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"for every word in the tokens, iterate over the positional Index and get the file number and positions for a word.\n",
    "        Make a smaller dictionary containing only the files and position for the given words.\n",
    "    \"\"\"\n",
    "    for pos,word in enumerate(tokensWithoutStopWords):\n",
    "        for docId,positions in posIndex[PorterStemmer().stem(word)].items():\n",
    "\n",
    "            if docId not in indexForWord:\n",
    "                indexForWord[docId]=positions\n",
    "            else:\n",
    "                for i in positions:\n",
    "                    indexForWord[docId].append(i)\n",
    "    \n",
    "    todel =[]\n",
    "    \"\"\"For the file numbers and positions in the dictionary, remove the file number that contains positions less than the number of words in the query. \n",
    "        make a list that contains files with the same and pop them from the dictionary.\"\"\"\n",
    "    for k,v in indexForWord.items():\n",
    "        if(len(indexForWord[k])<len(tokensWithoutStopWords)):\n",
    "            todel.append(k)\n",
    "\n",
    "    for i in todel:\n",
    "        indexForWord.pop(i)\n",
    "\n",
    "    \"\"\"Iterate over the file numbers and the positions in dictionary, sort the positions and check for consecutive positions.\n",
    "        If the number of consecutive postions is not empty then the file contains the query.\"\"\"\n",
    "    for docId,positions in indexForWord.items():\n",
    "        positions.sort()\n",
    "        consecutiveList = []\n",
    "        for i in range(len(positions)-1):\n",
    "            # if(positions[i+1]-positions[i] ==1):\n",
    "                consecutiveList.append(positions[i+1]-positions[i])\n",
    "\n",
    "        # print(positions)\n",
    "        \"\"\"Find the maximum number of consecutive 1s and if that is equal to length f tokens -1, add file\"\"\"\n",
    "        mx=0\n",
    "        for i in range(len(consecutiveList)):\n",
    "            c=0\n",
    "            while(i<len(consecutiveList) and consecutiveList[i]==1):\n",
    "                i+=1\n",
    "                c+=1\n",
    "            mx = max(c,mx)\n",
    "        if(mx== len(tokensWithoutStopWords)-1):\n",
    "            fileNames.append([docId,fileNumberMap[docId]])\n",
    "    \n",
    "        # print(consecutiveList)\n",
    "    print(\"Number Of Docs Retreived------>\\t\",len(fileNames),\"\\nFound  DocID & Name ------>\\t\",fileNames)\n",
    "\n",
    "\n",
    "phraseQuery(\"These artists are \\\"cultural jammers\\\"\")\n",
    "\n",
    "\n",
    "phraseQuery(\"laughing at your husband because \")\n",
    "\n",
    "phraseQuery(\"maiden\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
